{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TripData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_ILc1EPLa_EjwOPwW1uuxX1FWOt-xhES",
      "authorship_tag": "ABX9TyP3uuJLjWUKOvjgYrVGaXk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonRomaniello/CitibikeDocks/blob/master/TripData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKrcgrhCUpLa"
      },
      "source": [
        "# Cleaning and Merging Historical CitiBike Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oldMqCzyyj2O"
      },
      "source": [
        "CitiBike publishes trip reports every month to an AWS S3 bucket. These reports contain data of all the trips taken \n",
        "by CitiBike users, with information like the start times and locations, end times and locations, etc.\n",
        "\n",
        "These data hold information about the links between stations. Maybe there are some stations that are fed by a small handfull of other stations, perhaps each station has a wide reach. In order to be able to query an entire year (or more) of data at once, we need to create a datset containing multiple months.\n",
        "\n",
        "---\n",
        "\n",
        "*For a clean, code-only version that holds all the values in RAM before writing to disk, [see the other version here.](https://colab.research.google.com/drive/1IeBQ1JlIK4eveEzgd4NJVOCnKck8t0Wn?usp=sharing)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPfPSPYuL4T4"
      },
      "source": [
        "Notes: \n",
        "\n",
        "> PEP8 has been followed where I found it appropriate for the purposes of an interactive notebook. \n",
        "\n",
        "\n",
        "* Libraries are imported after the problem they are meant to solve has been introduced. I feel that this better reflects the creative process of tackling this dataset.\n",
        "\n",
        "* Double line function and class isolation has been ignored between cells. Cells are already separated visually, and text is often interspersed.\n",
        "\n",
        "\n",
        "> While CitiBike has stations on both sides of the Hudson, few (if any) rides originate in one state and end in another. There would be very little incentive to attempt this feat beyond bragging rights, and based on the two sets of trip data published depending on jurisdiction, it does not seem like anyone is doing it. As I live and work in New York City, I will only be focusing on New York.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TPDuWXApJYt"
      },
      "source": [
        "import requests\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xGGW8m5qcXy"
      },
      "source": [
        "# Dirty Zips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIY08Rn1yvDc"
      },
      "source": [
        "Unfortunately, some of the data are published as zip files that also contain MacOS special files, which means PANDAS can't simply ingest the zip file as published.\n",
        "\n",
        "We will use Requests to grab the file from the S3 bucket, BytesIO to keep the zip directory in memory, and ZipFile to work with the zip directory to extract the CSV only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzeoP1gH2hxo"
      },
      "source": [
        "from io import BytesIO\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxPQznGlsztx",
        "outputId": "0e182dfc-661a-436f-8b13-cc0e99d2cb18"
      },
      "source": [
        "dirtyZipUrl = 'https://s3.amazonaws.com/tripdata/202108-citibike-tripdata.csv.zip'\n",
        "dirtyZipFilename = requests.get(dirtyZipUrl).content\n",
        "dirtyZipFile = ZipFile( BytesIO(dirtyZipFilename), 'r')\n",
        "\n",
        "for item in dirtyZipFile.namelist():\n",
        "  print(\"File in zip:\" + item)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File in zip:202108-citibike-tripdata.csv\n",
            "File in zip:__MACOSX/._202108-citibike-tripdata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0QiLCnH2DIN"
      },
      "source": [
        "There it is, the stuff that PANDAS doesn't like. The files in the \"__MACOSX\" directory will cause the PANDAS read_csv() function to throw an exception.\n",
        "\n",
        "Not all of the published zip directories have this problem, but we shoud get rid of it if it is in there.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnbDFrexxe8b"
      },
      "source": [
        "justCSV = [cleanFilename \n",
        "           for cleanFilename in dirtyZipFile.namelist() \n",
        "           if \"._\" not in cleanFilename \n",
        "           and \".csv\" \n",
        "           in cleanFilename][0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orzG8pcU3_94"
      },
      "source": [
        "And now we can load the data and make sure it is as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "ekjVGeq0FPad",
        "outputId": "44820b23-bfa3-4b19-ef3d-9231f5769d34"
      },
      "source": [
        "tripData = pd.read_csv(dirtyZipFile.open(justCSV), low_memory=False)\n",
        "tripData.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ride_id</th>\n",
              "      <th>rideable_type</th>\n",
              "      <th>started_at</th>\n",
              "      <th>ended_at</th>\n",
              "      <th>start_station_name</th>\n",
              "      <th>start_station_id</th>\n",
              "      <th>end_station_name</th>\n",
              "      <th>end_station_id</th>\n",
              "      <th>start_lat</th>\n",
              "      <th>start_lng</th>\n",
              "      <th>end_lat</th>\n",
              "      <th>end_lng</th>\n",
              "      <th>member_casual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FB6B89D05B67EBED</td>\n",
              "      <td>classic_bike</td>\n",
              "      <td>2021-08-24 15:59:57</td>\n",
              "      <td>2021-08-24 16:42:07</td>\n",
              "      <td>Broadway &amp; E 21 St</td>\n",
              "      <td>6098.10</td>\n",
              "      <td>Central Park North &amp; Adam Clayton Powell Blvd</td>\n",
              "      <td>7617.07</td>\n",
              "      <td>40.739888</td>\n",
              "      <td>-73.989586</td>\n",
              "      <td>40.799484</td>\n",
              "      <td>-73.955613</td>\n",
              "      <td>member</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E13DA3E30CEF8DFC</td>\n",
              "      <td>classic_bike</td>\n",
              "      <td>2021-08-18 13:12:01</td>\n",
              "      <td>2021-08-18 13:21:26</td>\n",
              "      <td>E 13 St &amp; 2 Ave</td>\n",
              "      <td>5820.08</td>\n",
              "      <td>Henry St &amp; Grand St</td>\n",
              "      <td>5294.04</td>\n",
              "      <td>40.731539</td>\n",
              "      <td>-73.985302</td>\n",
              "      <td>40.714211</td>\n",
              "      <td>-73.981095</td>\n",
              "      <td>member</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56617490AB8AE69C</td>\n",
              "      <td>classic_bike</td>\n",
              "      <td>2021-08-17 14:31:23</td>\n",
              "      <td>2021-08-17 14:35:34</td>\n",
              "      <td>E 95 St &amp; 3 Ave</td>\n",
              "      <td>7365.13</td>\n",
              "      <td>E 84 St &amp; Park Ave</td>\n",
              "      <td>7243.04</td>\n",
              "      <td>40.784903</td>\n",
              "      <td>-73.950503</td>\n",
              "      <td>40.778627</td>\n",
              "      <td>-73.957721</td>\n",
              "      <td>member</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CA908B271C7D6663</td>\n",
              "      <td>classic_bike</td>\n",
              "      <td>2021-08-11 10:00:12</td>\n",
              "      <td>2021-08-11 10:31:01</td>\n",
              "      <td>Madison Ave &amp; E 82 St</td>\n",
              "      <td>7188.13</td>\n",
              "      <td>E 84 St &amp; Park Ave</td>\n",
              "      <td>7243.04</td>\n",
              "      <td>40.778131</td>\n",
              "      <td>-73.960694</td>\n",
              "      <td>40.778627</td>\n",
              "      <td>-73.957721</td>\n",
              "      <td>casual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3E170CE1F4FE179D</td>\n",
              "      <td>classic_bike</td>\n",
              "      <td>2021-08-12 19:28:38</td>\n",
              "      <td>2021-08-12 19:48:50</td>\n",
              "      <td>E 74 St &amp; 1 Ave</td>\n",
              "      <td>6953.08</td>\n",
              "      <td>E 84 St &amp; Park Ave</td>\n",
              "      <td>7243.04</td>\n",
              "      <td>40.768974</td>\n",
              "      <td>-73.954823</td>\n",
              "      <td>40.778627</td>\n",
              "      <td>-73.957721</td>\n",
              "      <td>casual</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            ride_id rideable_type  ...    end_lng member_casual\n",
              "0  FB6B89D05B67EBED  classic_bike  ... -73.955613        member\n",
              "1  E13DA3E30CEF8DFC  classic_bike  ... -73.981095        member\n",
              "2  56617490AB8AE69C  classic_bike  ... -73.957721        member\n",
              "3  CA908B271C7D6663  classic_bike  ... -73.957721        casual\n",
              "4  3E170CE1F4FE179D  classic_bike  ... -73.957721        casual\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elZppxMZlqpT"
      },
      "source": [
        "Great.\n",
        "\n",
        "We should turn this process into a function that takes the URL of the S3 item as input and returns a pandas DataFrame, because we will be doing this many times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WfuK40xEvq8"
      },
      "source": [
        "def readDirtyZip(dirtyZipUrl):\n",
        "  dirtyZipFilename = requests.get(dirtyZipUrl).content\n",
        "  dirtyZipFile = ZipFile( BytesIO(dirtyZipFilename), 'r')\n",
        "  tripData = pd.read_csv(dirtyZipFile.open([cleanFilename \n",
        "                                            for cleanFilename in dirtyZipFile.namelist() \n",
        "                                            if \"._\" not in cleanFilename \n",
        "                                            and \".csv\" \n",
        "                                            in cleanFilename][0]),\n",
        "                                            low_memory=False)\n",
        "  \n",
        "  return tripData"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBQ3P5u7Zcq1"
      },
      "source": [
        "# Legacy Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nysVlDtTKZdZ"
      },
      "source": [
        "Before going any further in creating our trip dataset, there is a slight wrinkle. At some point CitiBike changed the IDs for the all the stations. \n",
        "\n",
        "Graciously, they saw fit to include the old names *and* new names in the JSON feed that provides live information about the system.\n",
        "\n",
        "This will allow us to construct a dictionary that which we can use to rename the old trip data to reflect the current naming paradigm.\n",
        "\n",
        "Notes:  \n",
        "\n",
        "*   Stations that begin with letters include stations in New Jersey, so we will remove them when we make the dictionary.\n",
        "* The legacy system used int64 as the datatype for station IDs. The new system uses strings. When constructing the dictionary, the legacy IDs need to be type cast.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCHD7z9OE0N1"
      },
      "source": [
        "stationLocationsRequest = requests.get('https://gbfs.citibikenyc.com/gbfs/en/station_information.json')\n",
        "stationLocationData = stationLocationsRequest.json()\n",
        "stationLocations = pd.DataFrame(stationLocationData['data']['stations'])\n",
        "stationNameDictionary = dict(zip(stationLocations[stationLocations['short_name'].str.contains('[a-zA-Z]+',\n",
        "                                                            regex=True)==False].legacy_id.astype('int64'),\n",
        "                                                            stationLocations[stationLocations['short_name'].str.contains('[a-zA-Z]+',\n",
        "                                                            regex=True)==False].short_name))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KkKx1-bPXc3"
      },
      "source": [
        "We don't need anything except the dictionary, so we will delete everything else that went into creating the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-32gckvPS_P"
      },
      "source": [
        "del stationLocationsRequest, stationLocationData, stationLocations"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E13_pE8R5c9k"
      },
      "source": [
        "Since we are trying to predict the availability of bikes and open docks in the current CitiBike system, the new names will be used to rename old trip station IDs.\n",
        "\n",
        "The last month that used the legacy IDs appears to be January, 2021. We should test our renaming dictionary on this before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "nYBkJ1wY8nSa",
        "outputId": "c7a7b091-a071-4e08-9450-df42cb38438a"
      },
      "source": [
        "legacyTrips = readDirtyZip('https://s3.amazonaws.com/tripdata/202101-citibike-tripdata.csv.zip')\n",
        "legacyTrips['start station id'] = legacyTrips['start station id'].map(stationNameDictionary)\n",
        "legacyTrips['end station id'] = legacyTrips['end station id'].map(stationNameDictionary)\n",
        "legacyTrips.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tripduration</th>\n",
              "      <th>starttime</th>\n",
              "      <th>stoptime</th>\n",
              "      <th>start station id</th>\n",
              "      <th>start station name</th>\n",
              "      <th>start station latitude</th>\n",
              "      <th>start station longitude</th>\n",
              "      <th>end station id</th>\n",
              "      <th>end station name</th>\n",
              "      <th>end station latitude</th>\n",
              "      <th>end station longitude</th>\n",
              "      <th>bikeid</th>\n",
              "      <th>usertype</th>\n",
              "      <th>birth year</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2513</td>\n",
              "      <td>2021-01-01 00:00:11.9020</td>\n",
              "      <td>2021-01-01 00:42:05.2260</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>Underhill Ave &amp; Lincoln Pl</td>\n",
              "      <td>40.674012</td>\n",
              "      <td>-73.967146</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>Underhill Ave &amp; Lincoln Pl</td>\n",
              "      <td>40.674012</td>\n",
              "      <td>-73.967146</td>\n",
              "      <td>47812</td>\n",
              "      <td>Customer</td>\n",
              "      <td>1969</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2519</td>\n",
              "      <td>2021-01-01 00:00:15.0960</td>\n",
              "      <td>2021-01-01 00:42:14.9780</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>Underhill Ave &amp; Lincoln Pl</td>\n",
              "      <td>40.674012</td>\n",
              "      <td>-73.967146</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>Underhill Ave &amp; Lincoln Pl</td>\n",
              "      <td>40.674012</td>\n",
              "      <td>-73.967146</td>\n",
              "      <td>47571</td>\n",
              "      <td>Customer</td>\n",
              "      <td>1969</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1207</td>\n",
              "      <td>2021-01-01 00:00:28.9300</td>\n",
              "      <td>2021-01-01 00:20:36.6510</td>\n",
              "      <td>7188.10</td>\n",
              "      <td>E 81 St &amp; Park Ave</td>\n",
              "      <td>40.776777</td>\n",
              "      <td>-73.959010</td>\n",
              "      <td>6912.01</td>\n",
              "      <td>7 Ave &amp; Central Park South</td>\n",
              "      <td>40.766741</td>\n",
              "      <td>-73.979069</td>\n",
              "      <td>37451</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>2002</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2506</td>\n",
              "      <td>2021-01-01 00:00:32.7130</td>\n",
              "      <td>2021-01-01 00:42:19.3980</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>Underhill Ave &amp; Lincoln Pl</td>\n",
              "      <td>40.674012</td>\n",
              "      <td>-73.967146</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>Underhill Ave &amp; Lincoln Pl</td>\n",
              "      <td>40.674012</td>\n",
              "      <td>-73.967146</td>\n",
              "      <td>48884</td>\n",
              "      <td>Customer</td>\n",
              "      <td>2002</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>959</td>\n",
              "      <td>2021-01-01 00:00:35.3650</td>\n",
              "      <td>2021-01-01 00:16:34.6010</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Water - Whitehall Plaza</td>\n",
              "      <td>40.702551</td>\n",
              "      <td>-74.012723</td>\n",
              "      <td>5181.04</td>\n",
              "      <td>Cherry St</td>\n",
              "      <td>40.712199</td>\n",
              "      <td>-73.979481</td>\n",
              "      <td>26837</td>\n",
              "      <td>Customer</td>\n",
              "      <td>2002</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tripduration                 starttime  ... birth year gender\n",
              "0          2513  2021-01-01 00:00:11.9020  ...       1969      0\n",
              "1          2519  2021-01-01 00:00:15.0960  ...       1969      0\n",
              "2          1207  2021-01-01 00:00:28.9300  ...       2002      1\n",
              "3          2506  2021-01-01 00:00:32.7130  ...       2002      1\n",
              "4           959  2021-01-01 00:00:35.3650  ...       2002      1\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrPvm_oqO70Q"
      },
      "source": [
        "Looks good. In fact, looks great because station IDs that are not in the dictionary of current stations are replaced with NaN. We can use the PANDAS dropna fuction to remove them... later. First we will do a little more processing and cleaning.\n",
        "\n",
        "Some column names have changed in the new era. Spaces have been replaced with underscores in the new data, and the time stamp column names are prepositional phrases.\n",
        "\n",
        "We are only going to be using trip start times, end times, and the station IDs for the starting stations and end stations, so these are the only ones we will bother to rename."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em_5fjBPHhgK"
      },
      "source": [
        "legacyColumnRename = dict({'starttime': 'started_at',\n",
        "                           'stoptime': 'ended_at',\n",
        "                           'start station id': 'start_station_id',\n",
        "                           'end station id': 'end_station_id'})\n",
        "\n",
        "legacyTrips.rename(columns=legacyColumnRename, inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDpdpzm2Tah9"
      },
      "source": [
        "Then we can use the column renaming dictionary to cull the unwanted columns from our DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "jBkSA9pQTZTS",
        "outputId": "376b81d5-3ee1-450c-efeb-c9633ef10473"
      },
      "source": [
        "legacyTrips = legacyTrips[legacyColumnRename.values()]\n",
        "legacyTrips.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>started_at</th>\n",
              "      <th>ended_at</th>\n",
              "      <th>start_station_id</th>\n",
              "      <th>end_station_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-01 00:00:11.9020</td>\n",
              "      <td>2021-01-01 00:42:05.2260</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>4042.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-01-01 00:00:15.0960</td>\n",
              "      <td>2021-01-01 00:42:14.9780</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>4042.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-01-01 00:00:28.9300</td>\n",
              "      <td>2021-01-01 00:20:36.6510</td>\n",
              "      <td>7188.10</td>\n",
              "      <td>6912.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-01-01 00:00:32.7130</td>\n",
              "      <td>2021-01-01 00:42:19.3980</td>\n",
              "      <td>4042.08</td>\n",
              "      <td>4042.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-01-01 00:00:35.3650</td>\n",
              "      <td>2021-01-01 00:16:34.6010</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5181.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 started_at  ... end_station_id\n",
              "0  2021-01-01 00:00:11.9020  ...        4042.08\n",
              "1  2021-01-01 00:00:15.0960  ...        4042.08\n",
              "2  2021-01-01 00:00:28.9300  ...        6912.01\n",
              "3  2021-01-01 00:00:32.7130  ...        4042.08\n",
              "4  2021-01-01 00:00:35.3650  ...        5181.04\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKeHbz9nY3Xu"
      },
      "source": [
        "Drop NaNs...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MUZIxazXB89",
        "outputId": "51d84336-244b-49c1-f132-2cc86833fe35"
      },
      "source": [
        "legacyTrips.dropna(inplace=True)\n",
        "legacyTrips.isna().sum()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "started_at          0\n",
              "ended_at            0\n",
              "start_station_id    0\n",
              "end_station_id      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1ibVALhu_Nw"
      },
      "source": [
        "Had we done this before trimming the columns we might have lost desired data if there was mssing information in columns that we aren't even going to be using in the final trip dataset.\n",
        "\n",
        "We should also convert the local timestamps into a Unix timestamp, as minute-by-minute data from the JSON feed are timestamped with the seconds since the epoch. We will use the Python time library, and the Pandas apply function with a lamba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cObwSddQkWI-"
      },
      "source": [
        "import time"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6CZNjTXsCil"
      },
      "source": [
        "Here is an example of the time stamp with the calendar date and time as recorded in the published data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq7x5VPCpcRx",
        "outputId": "ce1933da-4f02-447d-dab4-53829fefb052"
      },
      "source": [
        "print(legacyTrips.iloc[0,0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-01-01 00:00:11.9020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcYkkPGNsahN"
      },
      "source": [
        "We will be using time.strtime to convert a local time into a time struct, and then using time.mktime to convert this into seconds since the epoch.\n",
        "\n",
        "The legacy data includes millisecond information after the decimal place, the more recently published data does not. The first 19 characters of the string the time since the epoch in both formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uAGHQi8l-Jf",
        "outputId": "7c1382f3-2c2c-452f-bb6b-39797a2aee5e"
      },
      "source": [
        "print(int(time.mktime(time.strptime(legacyTrips.iloc[0,0][:19],\n",
        "                                \"%Y-%m-%d %H:%M:%S\"))\n",
        "                                 ))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1609459211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXon4Qy5kW7o"
      },
      "source": [
        "Great.\n",
        "\n",
        "We can wrap this all into a function that accepts a URL of an S3 resource, checks whether any formatting is needed, performs the changes, and returns a cleaned and formatted DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smbZ-BJtu8Fu"
      },
      "source": [
        "def legacyCheckFix(s3URL):\n",
        "  legacyTrips = readDirtyZip(s3URL)\n",
        "\n",
        "  if 'start station id' in legacyTrips.columns:\n",
        "    legacyTrips['start station id'] = legacyTrips['start station id'].map(stationNameDictionary)\n",
        "    legacyTrips['end station id'] = legacyTrips['end station id'].map(stationNameDictionary)\n",
        "    legacyTrips.rename(columns=legacyColumnRename, inplace=True)\n",
        "  \n",
        "  legacyTrips = legacyTrips[legacyColumnRename.values()]\n",
        "  legacyTrips.dropna(inplace=True)\n",
        "\n",
        "  legacyTrips['started_at'] = legacyTrips['started_at'].apply(lambda timestamp: \n",
        "                              int(time.mktime(time.strptime(timestamp[:19],\n",
        "                              \"%Y-%m-%d %H:%M:%S\"))))\n",
        "  \n",
        "  legacyTrips['ended_at'] = legacyTrips['ended_at'].apply(lambda timestamp: \n",
        "                              int(time.mktime(time.strptime(timestamp[:19],\n",
        "                              \"%Y-%m-%d %H:%M:%S\"))))\n",
        "  \n",
        "  return legacyTrips"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO8sY8762EiF"
      },
      "source": [
        "# The S3 Bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PST-T3cuHxqX"
      },
      "source": [
        "We don't really want to go and find the URLs manually, so maybe a look at the contents of the bucket is in order. We will use Boto3 to do this, connecting to S3 without a signature to avoid having to configure anything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Id-yBfHAwR"
      },
      "source": [
        "%%capture\n",
        "!pip install boto3\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M7TfLmjIl9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ab4a38-4c37-4bfb-b451-e27bc35b3820"
      },
      "source": [
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "s3.list_objects(Bucket='tripdata')['Contents'][0:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'ETag': '\"b520a12de58eea58a3586f89bfcfbd9d-2\"',\n",
              "  'Key': '201306-citibike-tripdata.zip',\n",
              "  'LastModified': datetime.datetime(2018, 4, 30, 13, 18, 55, tzinfo=tzlocal()),\n",
              "  'Size': 16785103,\n",
              "  'StorageClass': 'STANDARD'},\n",
              " {'ETag': '\"7b3b260b2ab2e5349320121d04bd821c-22\"',\n",
              "  'Key': '201307-201402-citibike-tripdata.zip',\n",
              "  'LastModified': datetime.datetime(2017, 1, 18, 22, 23, 25, tzinfo=tzlocal()),\n",
              "  'Size': 178262576,\n",
              "  'StorageClass': 'STANDARD'},\n",
              " {'ETag': '\"dd3e6fd5f91715b31eae72868086c08c-4\"',\n",
              "  'Key': '201307-citibike-tripdata.zip',\n",
              "  'LastModified': datetime.datetime(2017, 1, 18, 22, 23, 27, tzinfo=tzlocal()),\n",
              "  'Size': 27074629,\n",
              "  'StorageClass': 'STANDARD'}]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On6QNxNssAzb"
      },
      "source": [
        "It looks like the 'LastModified' values aren't reliably correlated with the time period covered by the collected trip data.\n",
        "\n",
        "The 'Key' key, which returns the name of a zip directory, is what we want.\n",
        "\n",
        "If we provide a starting month and year and an ending month, we can get a list of all the URLs that correspond to the published trip data for that time span.\n",
        "\n",
        "Instead of a function, this time a class makes more sense. Included in the class is a filename generator, more on that later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqjvCtlqaf7a"
      },
      "source": [
        "class fileListUrls:\n",
        "\n",
        "  def __init__(self, startMonth, startYear, endMonth, endYear):\n",
        "    self.startMonth = startMonth\n",
        "    self.startYear = startYear\n",
        "    self.endMonth = endMonth\n",
        "    self.endYear = endYear\n",
        "\n",
        "  def tripURLs(self):\n",
        "    tripURLs = []\n",
        "    monthRange = pd.date_range((str(self.startYear) + '-' + str(self.startMonth)),\n",
        "                               (str(self.endYear) + '-' + str(self.endMonth)) ,\n",
        "                               freq='MS').strftime(\"%Y%m\").tolist()\n",
        "                               \n",
        "    for dictName in s3.list_objects(Bucket='tripdata')['Contents']:\n",
        "      for month in monthRange:\n",
        "        if dictName['Key'].startswith(month):\n",
        "          tripURLs.append('https://s3.amazonaws.com/tripdata/' + dictName['Key'])\n",
        "          monthRange.remove(month)\n",
        "  \n",
        "    tripURLs.reverse()\n",
        "    return tripURLs\n",
        "\n",
        "  def nameForCsv(self):\n",
        "    nameForCsv = ('/drive/MyDrive/' \n",
        "                  + str(self.startYear) \n",
        "                  + str(self.startMonth).zfill(2)\n",
        "                  + '-' \n",
        "                  + str(self.endYear) \n",
        "                  + str(self.endMonth).zfill(2)\n",
        "                  + '.csv')\n",
        "    \n",
        "    return nameForCsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Mf8I763Shs"
      },
      "source": [
        "Putting it all together, if we provide a starting month and year, an ending month and year, we can grab all of the trip data in that range, clean it, and merge it into one large dataset.\n",
        "\n",
        "In this example I'm writing the results the base directory in my Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKVv1NBSHobM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuf96DirAEEQ"
      },
      "source": [
        "# RAM optimized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgLWkOYRAPEM"
      },
      "source": [
        "If RAM is limited, this code writes to disk after each month is processed, appending to the CSV created during the first pass.\n",
        "\n",
        "It is slow, but is only limited by disk space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPtr93XWQpwP"
      },
      "source": [
        "urlRange = fileListUrls(input(\"Start month (integer): \"),\n",
        "                        input(\"Start year: \"),\n",
        "                        input(\"End month (integer, inclusive): \"),\n",
        "                        input(\"End year:\"))\n",
        "\n",
        "csvUrls = urlRange.tripURLs()\n",
        "\n",
        "hotTrips = legacyCheckFix(csvUrls[0])\n",
        "\n",
        "print(\"Writing first CSV...\")\n",
        "\n",
        "hotTrips.to_csv(urlRange.nameForCsv())\n",
        "\n",
        "del hotTrips\n",
        "\n",
        "for url in csvUrls[1:]:\n",
        "  hotTrips = legacyCheckFix(url)\n",
        "  print('Appending', url, 'to CSV.')\n",
        "  hotTrips.to_csv(urlRange.nameForCsv(), mode='a', header=False)\n",
        "  del hotTrips"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4aPTeRLBFBH"
      },
      "source": [
        "# Speed optimized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES2b1sFcBLcw"
      },
      "source": [
        "This version keeps the expanding trip DataFrame in RAM before finally writing to the disk.\n",
        "\n",
        "It is faster, but you could end up losing your progress if you run out of RAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_MfcfxjBJX0"
      },
      "source": [
        "urlRange = fileListUrls(input(\"Start month (integer): \"),\n",
        "                        input(\"Start year: \"),\n",
        "                        input(\"End month (integer, inclusive): \"),\n",
        "                        input(\"End year:\"))\n",
        "\n",
        "csvUrls = urlRange.tripURLs()\n",
        "\n",
        "headTrips = legacyCheckFix(csvUrls[0])\n",
        "\n",
        "for url in csvUrls[1:]:\n",
        "  tailTrips = legacyCheckFix(url)\n",
        "  print('Concatenating', url, \"...\")\n",
        "  headTrips = pd.concat([headTrips, tailTrips])\n",
        "\n",
        "print(\"Writing result to disk...\")\n",
        "headTrips.to_csv(urlRange.nameForCsv(),\n",
        "                 encoding=\"utf-8\",\n",
        "                 index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}